{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42af30bd",
   "metadata": {},
   "source": [
    "# LangChain + LangGraph workflow\n",
    "\n",
    "This section wraps the existing KG extraction and Neo4j write-up into a LangGraph workflow. It:\n",
    "- Builds `Document`s from sentences\n",
    "- Extracts a knowledge graph with `LLMGraphTransformer`\n",
    "- Writes it to Neo4j (with a helpful uniqueness constraint)\n",
    "- Runs quick sanity checks and prints results\n",
    "\n",
    "Requirements:\n",
    "- Environment variables: `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD`\n",
    "- Packages: `langchain-ollama`, `langchain-community`, `langchain-experimental`, `langgraph`, `neo4j`\n",
    "\n",
    "You can run the demo cell below or call `run_workflow([...])` with your own sentences.\n",
    "\n",
    "It assumes you have a local Ollama server installed with the `gemma3:27b-it-qat` model (or change the model name in the code):\n",
    "\n",
    "```bash\n",
    "OLLAMA_HOST=0.0.0.0 OLLAMA_CONTEXT_LENGTH=8192 OLLAMA_KV_CACHE_TYPE=q4_0 ollama start\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f92ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned & compact KG workflow\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, TypedDict, Optional, Tuple, Sequence\n",
    "\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import difflib\n",
    "from time import time\n",
    "from functools import lru_cache\n",
    "import yaml\n",
    "\n",
    "from joblib import Memory\n",
    "from tqdm.auto import tqdm\n",
    "import pywikibot\n",
    "from pywikibot.data.api import Request as ApiRequest\n",
    "import html\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Judge\n",
    "from t5_judge.judge import T5Judge\n",
    "\n",
    "# Config + Logging\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class KGConfig:\n",
    "    log_file: str = os.getenv(\"KG_LOG_FILE\", \"kg_workflow.log\")\n",
    "    log_format: str = os.getenv(\"KG_LOG_FORMAT\", \"%(asctime)s %(levelname)s - %(message)s\")\n",
    "    log_level: str = os.getenv(\"KG_LOG_LEVEL\", \"DEBUG\")\n",
    "    cache_dir: str = os.getenv(\"KG_CACHE_DIR\", \".kg_cache\")\n",
    "\n",
    "    llm_provider: str = os.getenv(\"LLM_PROVIDER\", \"openai\").lower()\n",
    "    openai_model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "    openai_api_key: Optional[str] = os.getenv(\"OPENAI_API_KEY\")\n",
    "    ollama_base_url: str = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n",
    "    ollama_model: str = os.getenv(\"OLLAMA_MODEL\", \"gemma3:27b-it-qat\")\n",
    "\n",
    "    judge_model: str = os.getenv(\"KG_JUDGE_MODEL\", \"google/flan-t5-large\")\n",
    "    judge_action: str = os.getenv(\"KG_JUDGE_ACTION\", \"filter\")  # or \"annotate\"\n",
    "\n",
    "    wiki_lang_default: str = os.getenv(\"KG_WIKI_LANG\", \"en\")\n",
    "\n",
    "\n",
    "def setup_logging(cfg: KGConfig) -> logging.Logger:\n",
    "    Path(cfg.log_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=cfg.log_file,\n",
    "        format=cfg.log_format,\n",
    "        level=getattr(logging, cfg.log_level.upper(), logging.DEBUG),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    if os.getenv(\"KG_LOG_CONSOLE\", \"0\") == \"1\":\n",
    "        console = logging.StreamHandler()\n",
    "        console.setFormatter(logging.Formatter(cfg.log_format))\n",
    "        logging.getLogger().addHandler(console)\n",
    "    return logging.getLogger(\"kg_disambiguation\")\n",
    "\n",
    "\n",
    "CFG = KGConfig()\n",
    "logger = setup_logging(CFG)\n",
    "\n",
    "# Caching for expensive ops\n",
    "memory = Memory(location=CFG.cache_dir, verbose=0)\n",
    "\n",
    "# Make pywikibot happy for read-only ops\n",
    "os.environ.setdefault(\"PYWIKIBOT_NO_USER_CONFIG\", \"1\")\n",
    "\n",
    "# State definition\n",
    "\n",
    "class KGState(TypedDict, total=False):\n",
    "    sentences: List[str]\n",
    "    docs: List[Document]\n",
    "    graph_docs: List[Any]\n",
    "    node_samples: List[Dict[str, Any]]\n",
    "    rel_samples: List[Dict[str, Any]]\n",
    "    summary: Dict[str, Any]\n",
    "    # Disambiguation config + results\n",
    "    disambiguate_labels: List[str]\n",
    "    disambiguation_threshold: float\n",
    "    canonical_map: Dict[str, Dict[str, str]]  # label -> {original_name: canonical_name}\n",
    "    # Wikipedia grounding config\n",
    "    ground_labels: List[str]\n",
    "    wiki_language: str\n",
    "    wiki_search_k: int\n",
    "    wiki_match_threshold: float\n",
    "    wiki_use_deepcat: bool\n",
    "    wiki_query_hints: Dict[str, str]\n",
    "    # Chunking\n",
    "    chunk_target_bytes: int\n",
    "    # Judge config + results\n",
    "    judge_node_threshold: float\n",
    "    judge_rel_threshold: float\n",
    "    judge_model_name: str\n",
    "    judge_device: str\n",
    "    judge_batch_size: int\n",
    "    judge_action: str  # 'filter' | 'annotate'\n",
    "    judge_stats: Dict[str, Any]\n",
    "    judge_results: List[Dict[str, Any]]\n",
    "\n",
    "# Extraction config\n",
    "\n",
    "allowed_nodes = [\n",
    "    \"Person\",\n",
    "    \"Organization\",\n",
    "    \"Product\",\n",
    "    \"Location\",\n",
    "    \"City\",\n",
    "    \"Country\",\n",
    "    \"Government Body\",\n",
    "    \"Year\",\n",
    "]\n",
    "\n",
    "allowed_relationships = [\n",
    "    (\"Person\", \"WORKS_FOR\", \"Organization\"),\n",
    "    (\"Person\", \"FOUNDED\", \"Organization\"),\n",
    "    (\"Organization\", \"ACQUIRED\", \"Organization\"),\n",
    "    (\"Organization\", \"LOCATED_IN\", \"Location\"),\n",
    "    (\"Person\", \"AFFILIATED_WITH\", \"Organization\"),\n",
    "    (\"Organization\", \"COMPETES_WITH\", \"Organization\"),\n",
    "    (\"Product\", \"MADE_BY\", \"Organization\"),\n",
    "    (\"Organization\", \"PARTNERED_WITH\", \"Organization\"),\n",
    "    (\"Product\", \"COMPETES_WITH\", \"Product\"),\n",
    "]\n",
    "\n",
    "node_properties = [\n",
    "    \"name\",\n",
    "    \"wiki_title\",\n",
    "    \"wiki_pageid\",\n",
    "    \"wiki_url\",\n",
    "    \"wiki_lang\",\n",
    "    \"wiki_description\",\n",
    "    \"wikidata_id\",\n",
    "    \"judge_score\",\n",
    "]\n",
    "\n",
    "additional_instructions = (\n",
    "    \"\"\"\n",
    "When extracting nodes, always include a 'name' property which should contain the text extracted.\n",
    "\n",
    "Here are the definitions of the allowed node types:\n",
    "\n",
    "- Person: An individual named human being, e.g., \"John Doe\", \"Alice Smith\".\n",
    "- Organization: A group of people working together, e.g., \"Apple Inc\", \"NASA\".\n",
    "- Product: A specific item produced for sale, e.g., \"iPhone\", \"Microsoft Windows\".\n",
    "- Location: A specific place, e.g., \"factory\", or \"office\".\n",
    "- City: A large town, e.g., \"New York\", \"San Francisco\".\n",
    "- Country: A nation with its own government, e.g., \"United States\", \"Canada\".\n",
    "- Government Body: An organization that governs a specific area, e.g., \"United Nations\", \"Federal Reserve\".\n",
    "- Year: A specific year in time, e.g., \"2020\", \"1999\".\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# LLM factory + transformer\n",
    "\n",
    "def make_llm(cfg: KGConfig):\n",
    "    if cfg.llm_provider == \"openai\":\n",
    "        return ChatOpenAI(model=cfg.openai_model, temperature=0, openai_api_key=cfg.openai_api_key)\n",
    "    return ChatOllama(base_url=cfg.ollama_base_url, model=cfg.ollama_model, temperature=0)\n",
    "\n",
    "\n",
    "llm = make_llm(CFG)\n",
    "\n",
    "transformer = LLMGraphTransformer(\n",
    "    llm=llm,\n",
    "    allowed_nodes=allowed_nodes,\n",
    "    allowed_relationships=allowed_relationships,\n",
    "    node_properties=node_properties,\n",
    "    additional_instructions=additional_instructions,\n",
    "    strict_mode=True,\n",
    ")\n",
    "\n",
    "# Small helpers\n",
    "\n",
    "def _slugify(text: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"-\", text.lower()).strip(\"-\")\n",
    "\n",
    "\n",
    "def _normalized_id(name: str, label: str) -> str:\n",
    "    return f\"{_slugify(name)}::{label}\"\n",
    "\n",
    "\n",
    "# Attr-or-dict tolerant accessors (kept to preserve compatibility with LangChain types)\n",
    "\n",
    "def _get(obj: Any, name: str, default=None):\n",
    "    if hasattr(obj, name):\n",
    "        return getattr(obj, name)\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(name, default)\n",
    "    return default\n",
    "\n",
    "\n",
    "def _set(obj: Any, name: str, value: Any):\n",
    "    if hasattr(obj, name):\n",
    "        setattr(obj, name, value)\n",
    "    elif isinstance(obj, dict):\n",
    "        obj[name] = value\n",
    "\n",
    "\n",
    "def _ensure_node_props(node: Any) -> Dict[str, Any]:\n",
    "    props = _get(node, \"properties\")\n",
    "    if not isinstance(props, dict):\n",
    "        props = {}\n",
    "        _set(node, \"properties\", props)\n",
    "    return props\n",
    "\n",
    "\n",
    "def _ensure_rel_props(rel: Any) -> Dict[str, Any]:\n",
    "    props = _get(rel, \"properties\")\n",
    "    if not isinstance(props, dict):\n",
    "        props = {}\n",
    "        _set(rel, \"properties\", props)\n",
    "    return props\n",
    "\n",
    "\n",
    "def _get_node_label(node: Any) -> Optional[str]:\n",
    "    return _get(node, \"type\")\n",
    "\n",
    "\n",
    "def _get_node_id(node: Any) -> Optional[str]:\n",
    "    return _get(node, \"id\")\n",
    "\n",
    "\n",
    "def _get_node_name(node: Any) -> Optional[str]:\n",
    "    props = _get(node, \"properties\") or {}\n",
    "    if isinstance(props, dict):\n",
    "        name = props.get(\"name\")\n",
    "        if name:\n",
    "            return name\n",
    "    return _get_node_id(node)\n",
    "\n",
    "\n",
    "def _set_node_name(node: Any, new_name: str):\n",
    "    props = _ensure_node_props(node)\n",
    "    props[\"name\"] = new_name\n",
    "\n",
    "\n",
    "# Judge templates\n",
    "ENTITY_TYPE_TEMPLATES: Dict[str, str] = {\n",
    "    \"Person\": \"is a person.\",\n",
    "    \"Organization\": \"is an organization.\",\n",
    "    \"Product\": \"is a product.\",\n",
    "    \"Location\": \"is a location.\",\n",
    "    \"City\": \"is a city.\",\n",
    "    \"Country\": \"is a country.\",\n",
    "    \"Government Body\": \"is a government body.\",\n",
    "    \"Year\": \"is a year.\",\n",
    "}\n",
    "\n",
    "RELATION_TEMPLATES: Dict[str, str] = {\n",
    "    \"WORKS_FOR\": \"{src} works for {tgt}.\",\n",
    "    \"FOUNDED\": \"{src} founded {tgt}.\",\n",
    "    \"ACQUIRED\": \"{src} acquired {tgt}.\",\n",
    "    \"LOCATED_IN\": \"{src} is located in {tgt}.\",\n",
    "    \"AFFILIATED_WITH\": \"{src} is affiliated with {tgt}.\",\n",
    "    \"COMPETES_WITH\": \"{src} competes with {tgt}.\",\n",
    "    \"MADE_BY\": \"{src} is made by {tgt}.\",\n",
    "    \"PARTNERED_WITH\": \"{src} partnered with {tgt}.\",\n",
    "}\n",
    "\n",
    "DEFAULT_WIKI_CATEGORY: Dict[str, str] = {\n",
    "    \"Organization\": \"Companies\",\n",
    "    \"Product\": \"Products\",\n",
    "    \"Person\": \"People\",\n",
    "    \"Country\": \"Countries\",\n",
    "    \"Government Body\": \"Government bodies\",\n",
    "    \"Location\": \"Places\",\n",
    "}\n",
    "\n",
    "# Optional: Reranker (clean + lazy)\n",
    "\n",
    "try:\n",
    "    from rerankers import Reranker\n",
    "    from rerankers import Document as RRDocument\n",
    "except Exception as e:\n",
    "    Reranker = None\n",
    "    RRDocument = None\n",
    "    logger.debug(\"'rerankers' not available: %s\", e)\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def _get_wiki_reranker(model_name: Optional[str] = None):\n",
    "    if Reranker is None:\n",
    "        return None\n",
    "    name = model_name or os.getenv(\"RERANKERS_MODEL\", \"mixedbread-ai/mxbai-rerank-base-v1\")\n",
    "    try:\n",
    "        return Reranker(name, model_type=\"cross-encoder\", verbose=0)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Reranker init failed: %s\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# Wikipedia helpers\n",
    "\n",
    "@memory.cache\n",
    "def wiki_search_titles(query: str, lang: str, limit: int, hint: Optional[str], use_deepcat: bool) -> List[Dict[str, Any]]:\n",
    "    if not query:\n",
    "        return []\n",
    "    try:\n",
    "        pywikibot.config.user_agent = \"PedroSearchBot/1.0\"\n",
    "\n",
    "        # Build the search string (CirrusSearch operators like intitle:, deepcat:)\n",
    "        safe = re.sub(r\"\\s+\", \" \", query.strip().replace('\"', \"\"))\n",
    "        eff = f'intitle:\"{safe}\"'\n",
    "        if use_deepcat and hint:\n",
    "            eff += f' deepcat:\"{hint}\"'\n",
    "        elif hint:\n",
    "            eff += f\" {hint}\"\n",
    "\n",
    "        site = pywikibot.Site(lang, \"wikipedia\")\n",
    "\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": eff,\n",
    "            \"srnamespace\": 0,\n",
    "            \"srlimit\": int(limit or 10),\n",
    "            # Ask specifically for snippet and other useful props\n",
    "            \"srprop\": \"snippet|size|wordcount|timestamp|titlesnippet\",\n",
    "            # Optional: relevance (default) vs justmatch/newest/oldest, etc.\n",
    "            # \"srsort\": \"relevance\",\n",
    "        }\n",
    "        data = ApiRequest(site=site, **params).submit()\n",
    "        hits = data.get(\"query\", {}).get(\"search\", []) or []\n",
    "\n",
    "        out: List[Dict[str, Any]] = []\n",
    "        for h in hits:\n",
    "            # Snippet comes HTML-escaped and with <span class=\"searchmatch\">…</span>\n",
    "            snippet_html = h.get(\"snippet\") or \"\"\n",
    "            out.append({\n",
    "                \"title\": h.get(\"title\"),\n",
    "                \"pageid\": h.get(\"pageid\"),\n",
    "                \"snippet\": html.unescape(snippet_html),\n",
    "                \"wordcount\": h.get(\"wordcount\"),\n",
    "                \"size\": h.get(\"size\"),\n",
    "                \"timestamp\": h.get(\"timestamp\"),\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Wikipedia title search failed for %r: %s\", query, e)\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "@memory.cache\n",
    "def wiki_page_info(title: str, lang: str) -> Dict[str, Any]:\n",
    "    try:\n",
    "        site = pywikibot.Site(lang, \"wikipedia\")\n",
    "        page = pywikibot.Page(site, title)\n",
    "        props = page.properties()\n",
    "        desc = page.description() if hasattr(page, \"description\") else None\n",
    "        return {\n",
    "            \"pageid\": page.pageid,\n",
    "            \"title\": page.title(),\n",
    "            \"fullurl\": page.full_url(),\n",
    "            \"description\": desc,\n",
    "            \"wikidata_id\": props.get(\"wikibase_item\"),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Wikipedia page info failed for title=%r: %s\", title, e)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def _best_wiki_match(query: str, results: List[Dict[str, Any]], thr: float, *, model_name: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Pick the best Wikipedia result.\n",
    "    Order of preference: exact slug -> reranker (if available) -> difflib fallback.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return None\n",
    "\n",
    "    q_slug = _slugify(query)\n",
    "    for r in results:  # fast path exact slug\n",
    "        if _slugify(r.get(\"title\", \"\")) == q_slug:\n",
    "            logger.debug(\"Exact slug match: %r\", r.get(\"title\"))\n",
    "            return r\n",
    "\n",
    "    # Try reranker if available\n",
    "    rr = _get_wiki_reranker(model_name)\n",
    "    if rr is not None and RRDocument is not None:\n",
    "        # Log all titles considered\n",
    "        logger.debug(\"Reranker candidates for %r: %s\", query, \", \".join([r.get(\"title\") for r in results if r.get(\"title\")]))\n",
    "        docs = [RRDocument(text=r[\"title\"]+\": \"+r[\"snippet\"], doc_id=str(i)) for i, r in enumerate(results) if r.get(\"title\")]\n",
    "        if docs:\n",
    "            try:\n",
    "                ranked = rr.rank(query=query, docs=docs) or []\n",
    "                if ranked:\n",
    "                    top = ranked[0]\n",
    "                    score = float(getattr(top, \"score\", 0.0))\n",
    "                    logger.debug(\"Reranker top: %r (%.4f)\", getattr(top, \"text\", None), score)\n",
    "                    if score >= thr:\n",
    "                        by_title = {r[\"title\"]: r for r in results if r.get(\"title\")}\n",
    "                        return by_title.get(getattr(top, \"text\", \"\"))\n",
    "            except (RuntimeError, ValueError, AttributeError) as e:\n",
    "                logger.debug(\"Reranker error, fallback to difflib: %s\", e)\n",
    "\n",
    "    # Fallback: difflib similarity\n",
    "    scored = []\n",
    "    ql = query.lower()\n",
    "    for r in results:\n",
    "        t = r.get(\"title\", \"\")\n",
    "        if not t:\n",
    "            continue\n",
    "        ratio = difflib.SequenceMatcher(a=ql, b=t.lower()).ratio()\n",
    "        scored.append((ratio, r))\n",
    "    if not scored:\n",
    "        return None\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    best_ratio, best = scored[0]\n",
    "    logger.debug(\"Fallback best: %r (%.4f)\", best.get(\"title\"), best_ratio)\n",
    "    return best if best_ratio >= thr else None\n",
    "\n",
    "\n",
    "# Relationship id remap\n",
    "\n",
    "def _remap_relationship_ids(doc: Any, id_map: Dict[str, str]) -> Dict[str, int]:\n",
    "    rel_src, rel_tgt = 0, 0\n",
    "    for rel in getattr(doc, \"relationships\", []):\n",
    "        src = _get(rel, \"source\"); tgt = _get(rel, \"target\")\n",
    "        src_id = _get(src, \"id\") if not isinstance(src, str) else src\n",
    "        tgt_id = _get(tgt, \"id\") if not isinstance(tgt, str) else tgt\n",
    "        if src_id in id_map:\n",
    "            if isinstance(src, str):\n",
    "                _set(rel, \"source\", id_map[src_id])\n",
    "            else:\n",
    "                _set(src, \"id\", id_map[src_id])\n",
    "            rel_src += 1\n",
    "        if tgt_id in id_map:\n",
    "            if isinstance(tgt, str):\n",
    "                _set(rel, \"target\", id_map[tgt_id])\n",
    "            else:\n",
    "                _set(tgt, \"id\", id_map[tgt_id])\n",
    "            rel_tgt += 1\n",
    "    return {\"rel_src\": rel_src, \"rel_tgt\": rel_tgt}\n",
    "\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def build_docs(state: KGState) -> KGState:\n",
    "    sentences = state.get(\"sentences\") or []\n",
    "    if not sentences:\n",
    "        raise ValueError(\"No sentences provided in state['sentences']\")\n",
    "    return {\"docs\": [Document(page_content=s, metadata={\"source\": \"workflow\"}) for s in sentences]}\n",
    "\n",
    "\n",
    "def extract_graph(state: KGState) -> KGState:\n",
    "    docs = state.get(\"docs\") or []\n",
    "    if not docs:\n",
    "        return {\"graph_docs\": [], \"summary\": {\"node_count\": 0, \"relationship_count\": 0}}\n",
    "\n",
    "    batch_size = int(state.get(\"extract_batch_size\") or os.getenv(\"KG_EXTRACT_BATCH_SIZE\", \"8\"))\n",
    "\n",
    "    graph_docs, errors = [], 0\n",
    "    with tqdm(total=len(docs), desc=\"Extract graph\", unit=\"doc\") as pbar:\n",
    "        for start in range(0, len(docs), batch_size):\n",
    "            batch = docs[start : start + batch_size]\n",
    "            try:\n",
    "                gds = transformer.convert_to_graph_documents(batch)\n",
    "                graph_docs.extend(gds)\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Failed to extract batch %d:%d: %s\", start, start + len(batch), e)\n",
    "                errors += len(batch)\n",
    "            finally:\n",
    "                pbar.update(len(batch))\n",
    "\n",
    "    summary = {\n",
    "        \"node_count\": sum(len(d.nodes) for d in graph_docs),\n",
    "        \"relationship_count\": sum(len(d.relationships) for d in graph_docs),\n",
    "        \"batch_size\": batch_size,\n",
    "    }\n",
    "    if errors:\n",
    "        summary[\"errors\"] = errors\n",
    "    return {\"graph_docs\": graph_docs, \"summary\": summary}\n",
    "\n",
    "# Doc chunker\n",
    "\n",
    "def chunk_docs(state: KGState) -> KGState:\n",
    "    \"\"\"\n",
    "    Split state['docs'] into semantically-meaningful ~1 KB chunks using semchunk.\n",
    "    - Targets ~256 tokens per chunk (≈ 1 KB at ~4 chars/token).\n",
    "    - Overlap ~10% (configurable).\n",
    "    - Preserves and augments metadata (parent doc tracking + byte offsets).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import semchunk\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"semchunk is required. Install with: pip install semchunk\"\n",
    "        ) from e\n",
    "\n",
    "    # Prefer a tokenizer that matches your LLM; fall back safely.\n",
    "    enc = None\n",
    "    try:\n",
    "        import tiktoken\n",
    "        # Try to match OpenAI model if used; otherwise fall back to cl100k_base\n",
    "        # (o200k_base suits GPT-4o/GPT-4.1 families; cl100k_base is broadly OK).\n",
    "        if CFG.llm_provider != \"openai\":\n",
    "            openai_model = None\n",
    "        else:\n",
    "            openai_model = CFG.openai_model\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(openai_model) if openai_model else tiktoken.get_encoding(\"cl100k_base\")\n",
    "        except Exception:\n",
    "            # 4o/4.1 often map to o200k_base; try that before final fallback\n",
    "            try:\n",
    "                enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "            except Exception:\n",
    "                enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        def token_counter(s: str) -> int:\n",
    "            return len(enc.encode(s))\n",
    "    except Exception:\n",
    "        # Minimal heuristic if tiktoken unavailable: ~4 chars/token\n",
    "        token_counter = lambda s: max(1, len(s) // 4)\n",
    "\n",
    "    # Config knobs (can be set in state)\n",
    "    target_bytes = int(state.get(\"chunk_target_bytes\") or 1024)        # ≈ 1 KB\n",
    "    # Approx chars≈bytes; tokens≈chars/4\n",
    "    chunk_size_tokens = int(state.get(\"chunk_size_tokens\") or max(1, target_bytes // 4))\n",
    "    overlap_tokens = state.get(\"chunk_overlap_tokens\")\n",
    "    if overlap_tokens is None:\n",
    "        overlap_tokens = max(1, int(0.1 * chunk_size_tokens))          # ~10% overlap\n",
    "\n",
    "    # Build the chunker\n",
    "    # semchunk.chunkerify accepts a tokenizer OR a token counter callable (+ size)\n",
    "    chunker = semchunk.chunkerify(token_counter, chunk_size=chunk_size_tokens)\n",
    "\n",
    "    docs = state.get(\"docs\") or []\n",
    "    if not docs:\n",
    "        return {\"docs\": []}\n",
    "\n",
    "    chunked_docs: List[Document] = []\n",
    "    for doc_idx, doc in enumerate(tqdm(docs, total=len(docs), desc=\"Chunk docs\", unit=\"doc\")):\n",
    "        text = getattr(doc, \"page_content\", \"\") or \"\"\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        # Ask for offsets so we can keep byte/char ranges\n",
    "        chunks, offsets = chunker(text, offsets=True, overlap=overlap_tokens)\n",
    "\n",
    "        parent_id = _slugify(f\"{_get(doc, 'metadata', {}).get('source', 'doc')}-{doc_idx}\")\n",
    "        base_meta = dict(getattr(doc, \"metadata\", {}) or {})\n",
    "        base_meta.setdefault(\"source\", base_meta.get(\"source\", \"workflow\"))\n",
    "        base_meta[\"parent_id\"] = parent_id\n",
    "        base_meta[\"parent_len\"] = len(text)\n",
    "        base_meta[\"chunk_size_tokens\"] = chunk_size_tokens\n",
    "        base_meta[\"chunk_overlap_tokens\"] = overlap_tokens\n",
    "\n",
    "        for i, (chunk, (start, end)) in enumerate(zip(chunks, offsets)):\n",
    "            md = dict(base_meta)\n",
    "            md.update({\n",
    "                \"chunk_index\": i,\n",
    "                \"chunk_start\": int(start),\n",
    "                \"chunk_end\": int(end),\n",
    "            })\n",
    "            chunked_docs.append(Document(page_content=chunk, metadata=md))\n",
    "\n",
    "    return {\"docs\": chunked_docs}\n",
    "\n",
    "# Judge\n",
    "\n",
    "# batched scoring with keep-on-error semantics\n",
    "def _batch_scores_safe(judge: T5Judge,\n",
    "                       pairs: Sequence[Tuple[str, str]],\n",
    "                       batch_size: int = 16) -> List[float]:\n",
    "    \"\"\"Return list of truth_scores for each (evidence, fact). On error, returns 0.0 for all.\"\"\"\n",
    "    try:\n",
    "        results = judge.batch_judge_claims(pairs, batch_size=batch_size)\n",
    "        return [float(r.truth_score) for r in results]\n",
    "    except (RuntimeError, ValueError) as e:\n",
    "        logger.debug(\"Judge batch error on %d pairs: %s\", len(pairs), e)\n",
    "        return [0.0] * len(pairs)\n",
    "\n",
    "\n",
    "def validate_with_judge(state: KGState) -> KGState:\n",
    "    node_thr = float(state.get(\"judge_node_threshold\") or 0.4)\n",
    "    rel_thr = float(state.get(\"judge_rel_threshold\") or 0.4)\n",
    "    model_name = state.get(\"judge_model_name\") or CFG.judge_model\n",
    "    device = state.get(\"judge_device\")\n",
    "    action = (state.get(\"judge_action\") or CFG.judge_action).lower()\n",
    "    keep_all = action == \"annotate\"\n",
    "    batch_size = int(state.get(\"judge_batch_size\") or 16)\n",
    "\n",
    "    try:\n",
    "        judge = T5Judge(model_name=model_name, device=device)\n",
    "    except Exception as e:\n",
    "        logger.warning(\"Judge initialization failed (%s). Skipping validation.\", e)\n",
    "        return {}\n",
    "\n",
    "    judge_results: List[Dict[str, object]] = []\n",
    "\n",
    "    docs = state.get(\"docs\") or []\n",
    "    gdocs = state.get(\"graph_docs\") or []\n",
    "    if not docs or not gdocs:\n",
    "        return {}\n",
    "\n",
    "    updated_docs: List[Any] = []\n",
    "    stats = {\"nodes_removed\": 0, \"rels_removed\": 0, \"nodes_kept\": 0, \"rels_kept\": 0, \"mode\": action}\n",
    "\n",
    "    for doc, gd in tqdm(zip(docs, gdocs), total=min(len(docs), len(gdocs)), desc=\"Judge validation\", unit=\"doc\"):\n",
    "        evidence = getattr(doc, \"page_content\", None) or \"\"\n",
    "\n",
    "        # NODES (batched)\n",
    "        node_items: List[Tuple[Any, str, str, str]] = []  # (node, label, name, fact_str)\n",
    "        node_pairs: List[Tuple[str, str]] = []            # (evidence, fact)\n",
    "        node_indices: List[int] = []                      # map back to node_items\n",
    "\n",
    "        # collect\n",
    "        for idx, node in enumerate(getattr(gd, \"nodes\", [])):\n",
    "            lbl = _get_node_label(node)\n",
    "            name = _get_node_name(node) or \"\"\n",
    "            if not lbl or not name:\n",
    "                if keep_all:\n",
    "                    # keep but without score\n",
    "                    logger.debug(\"Judge node: missing label or name; keeping as-is: %r\", node)\n",
    "                    _ensure_node_props(node).setdefault(\"judge_score\", 0.0)\n",
    "                    stats[\"nodes_kept\"] += 1\n",
    "                else:\n",
    "                    stats[\"nodes_removed\"] += 1\n",
    "                continue\n",
    "            tpl = ENTITY_TYPE_TEMPLATES.get(lbl)\n",
    "            if not tpl:\n",
    "                # unknown template -> keep as-is\n",
    "                stats[\"nodes_kept\"] += 1\n",
    "                continue\n",
    "            fact_str = f\"{name} {tpl}\"\n",
    "            node_items.append((node, lbl, name, fact_str))\n",
    "            node_pairs.append((evidence, fact_str))\n",
    "            node_indices.append(len(node_items) - 1)\n",
    "\n",
    "        # score batch\n",
    "        node_scores: List[float] = _batch_scores_safe(judge, node_pairs, batch_size=batch_size) if node_pairs else []\n",
    "\n",
    "        # apply decisions\n",
    "        new_nodes: List[Any] = []\n",
    "        valid_ids: set = set()\n",
    "        for (node, lbl, name, fact_str), score in zip(node_items, node_scores):\n",
    "            ok = (score >= node_thr) or keep_all\n",
    "            # log & annotate\n",
    "            judge_results.append({\n",
    "                'type': 'node',\n",
    "                'name': lbl,\n",
    "                'fact': fact_str,\n",
    "                'evidence': evidence,\n",
    "                'score': score,\n",
    "                'accepted': bool(score >= node_thr)\n",
    "            })\n",
    "            logger.debug(\"Judge node: %r %r -> %s (%.4f)\", lbl, name, \"ACCEPT\" if (score >= node_thr) else \"REJECT\", score)\n",
    "            _ensure_node_props(node)[\"judge_score\"] = score\n",
    "\n",
    "            if ok:\n",
    "                new_nodes.append(node)\n",
    "                nid = _get_node_id(node)\n",
    "                if nid:\n",
    "                    valid_ids.add(nid)\n",
    "                stats[\"nodes_kept\"] += 1\n",
    "            else:\n",
    "                stats[\"nodes_removed\"] += 1\n",
    "\n",
    "        # also carry over any previously kept nodes that weren’t scored (no tpl / missing name/label but kept in annotate)\n",
    "        # We already accounted stats for those above; ensure they remain in graph:\n",
    "        # Rebuild by iterating original nodes, keeping those that either were added as ok, or were kept earlier.\n",
    "        if keep_all:\n",
    "            # in annotate mode, keep all original nodes, but ensure judge_score is set for scored ones\n",
    "            original_nodes = getattr(gd, \"nodes\", [])\n",
    "            # avoid duplicates: use id set\n",
    "            kept_ids = { _get_node_id(n) for n in new_nodes if _get_node_id(n) }\n",
    "            for n in original_nodes:\n",
    "                nid = _get_node_id(n)\n",
    "                if nid and nid in kept_ids:\n",
    "                    continue\n",
    "                # if not already included, keep as-is (with possible default score)\n",
    "                _ensure_node_props(n).setdefault(\"judge_score\", 0.0)\n",
    "                new_nodes.append(n)\n",
    "                if nid:\n",
    "                    valid_ids.add(nid)\n",
    "\n",
    "        _set(gd, \"nodes\", new_nodes)\n",
    "\n",
    "        # RELATIONSHIPS (batched)\n",
    "        rel_items: List[Tuple[Any, str, str]] = []   # (rel, rtype, fact_str)\n",
    "        rel_pairs: List[Tuple[str, str]] = []\n",
    "\n",
    "        def _name_of(id_):\n",
    "            for n in new_nodes:\n",
    "                if _get_node_id(n) == id_:\n",
    "                    return _get_node_name(n) or id_\n",
    "            return id_\n",
    "\n",
    "        for rel in getattr(gd, \"relationships\", []):\n",
    "            rtype = _get(rel, \"type\") or _get(rel, \"label\")\n",
    "            src = _get(rel, \"source\"); tgt = _get(rel, \"target\")\n",
    "            src_id = _get(src, \"id\") if not isinstance(src, str) else src\n",
    "            tgt_id = _get(tgt, \"id\") if not isinstance(tgt, str) else tgt\n",
    "\n",
    "            if not rtype or not src_id or not tgt_id:\n",
    "                if keep_all:\n",
    "                    # keep, annotate later with 0.0 if needed\n",
    "                    _ensure_rel_props(rel).setdefault(\"judge_score\", 0.0)\n",
    "                    rel_items.append((rel, None, None))  # mark for unconditional keep\n",
    "                else:\n",
    "                    stats[\"rels_removed\"] += 1\n",
    "                continue\n",
    "\n",
    "            if not keep_all and (src_id not in valid_ids or tgt_id not in valid_ids):\n",
    "                stats[\"rels_removed\"] += 1\n",
    "                continue\n",
    "\n",
    "            tpl = RELATION_TEMPLATES.get(rtype)\n",
    "            if not tpl:\n",
    "                # keep as-is, no scoring\n",
    "                rel_items.append((rel, None, None))\n",
    "                continue\n",
    "\n",
    "            fact_str = tpl.format(src=_name_of(src_id), tgt=_name_of(tgt_id))\n",
    "            rel_items.append((rel, rtype, fact_str))\n",
    "            rel_pairs.append((evidence, fact_str))\n",
    "\n",
    "        rel_scores: List[float] = _batch_scores_safe(judge, rel_pairs, batch_size=batch_size) if rel_pairs else []\n",
    "\n",
    "        new_rels: List[Any] = []\n",
    "        score_iter = iter(rel_scores)\n",
    "        for rel, rtype, fact_str in rel_items:\n",
    "            if rtype is None:\n",
    "                # unscored but kept\n",
    "                new_rels.append(rel)\n",
    "                stats[\"rels_kept\"] += 1\n",
    "                continue\n",
    "\n",
    "            score = next(score_iter)\n",
    "            ok = (score >= rel_thr) or keep_all\n",
    "\n",
    "            judge_results.append({\n",
    "                'type': 'relation',\n",
    "                'name': rtype,\n",
    "                'fact': fact_str,\n",
    "                'evidence': evidence,\n",
    "                'score': score,\n",
    "                'accepted': bool(score >= rel_thr)\n",
    "            })\n",
    "            logger.debug(\"Judge rel: %r %r -> %s (%.4f)\", rtype, fact_str, \"ACCEPT\" if (score >= rel_thr) else \"REJECT\", score)\n",
    "            _ensure_rel_props(rel)[\"judge_score\"] = score\n",
    "\n",
    "            if ok:\n",
    "                new_rels.append(rel)\n",
    "                stats[\"rels_kept\"] += 1\n",
    "            else:\n",
    "                stats[\"rels_removed\"] += 1\n",
    "\n",
    "        _set(gd, \"relationships\", new_rels)\n",
    "        updated_docs.append(gd)\n",
    "\n",
    "    logger.info(\"Judge done (mode=%s). nodes_kept=%d nodes_removed=%d rels_kept=%d rels_removed=%d\",\n",
    "                action, stats[\"nodes_kept\"], stats[\"nodes_removed\"], stats[\"rels_kept\"], stats[\"rels_removed\"])\n",
    "    return {\"graph_docs\": updated_docs, \"judge_stats\": stats, \"judge_results\": judge_results}\n",
    "\n",
    "\n",
    "# Disambiguation (sentence-transformers)\n",
    "\n",
    "def disambiguate_entities(state: KGState) -> KGState:\n",
    "    labels = state.get(\"disambiguate_labels\") or [\"Product\"]\n",
    "    threshold = state.get(\"disambiguation_threshold\") or 0.85\n",
    "    logger.info(\"Disambiguation start: labels=%s threshold=%.2f\", labels, threshold)\n",
    "\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer, util\n",
    "    except Exception as e:\n",
    "        raise EnvironmentError(\n",
    "            \"sentence-transformers is required for disambiguation. Install with: pip install sentence-transformers\"\n",
    "        ) from e\n",
    "\n",
    "    # Build contextual mentions per label: each mention carries the surface name and a snippet of its source document.\n",
    "    docs = state.get(\"docs\") or []\n",
    "    gdocs = state.get(\"graph_docs\") or []\n",
    "\n",
    "    def _context_snippet(text: str, name: str, max_chars: int = 512) -> str:\n",
    "        try:\n",
    "            if not text:\n",
    "                return \"\"\n",
    "            t = text\n",
    "            needle = (name or \"\").lower()\n",
    "            idx = t.lower().find(needle) if needle else -1\n",
    "            if idx == -1:\n",
    "                return t[:max_chars]\n",
    "            half = max_chars // 2\n",
    "            start = max(0, idx - half)\n",
    "            end = min(len(t), idx + len(name or \"\") + half)\n",
    "            return t[start:end]\n",
    "        except Exception:\n",
    "            return (text or \"\")[:max_chars]\n",
    "\n",
    "    per_label_mentions: Dict[str, List[Dict[str, Any]]] = {lbl: [] for lbl in labels}\n",
    "    for i, gd in enumerate(gdocs):\n",
    "        evidence = getattr(docs[i], \"page_content\", \"\") if i < len(docs) else \"\"\n",
    "        for node in getattr(gd, \"nodes\", []):\n",
    "            label = _get_node_label(node)\n",
    "            if label in labels:\n",
    "                name = _get_node_name(node)\n",
    "                if not name:\n",
    "                    continue\n",
    "                ctx = _context_snippet(evidence, name, max_chars=512)\n",
    "                embed_text = f\"[{label}] {name}\\nContext: {ctx}\"\n",
    "                per_label_mentions[label].append({\"name\": name, \"embed_text\": embed_text})\n",
    "\n",
    "    if all(len(v) == 0 for v in per_label_mentions.values()):\n",
    "        logger.info(\"No names found for configured labels; skipping disambiguation.\")\n",
    "        return {\"canonical_map\": {}}\n",
    "\n",
    "    model = SentenceTransformer(\"Qwen/Qwen3-Embedding-4B\")\n",
    "\n",
    "    # Cluster contextual mentions and convert to a per-name canonical map by majority voting.\n",
    "    canonical_map: Dict[str, Dict[str, str]] = {lbl: {} for lbl in labels}\n",
    "    for label, mentions in per_label_mentions.items():\n",
    "        if not mentions:\n",
    "            continue\n",
    "        texts = [m[\"embed_text\"] for m in mentions]\n",
    "        logger.debug(\"Encoding %d mentions for label %s: %s\", len(texts), label, texts)\n",
    "        emb = model.encode(texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "        clusters = util.community_detection(emb, threshold=threshold, min_community_size=1)\n",
    "        logger.info(\"Label %s clustered into %d groups\", label, len(clusters))\n",
    "\n",
    "        # votes[name][canonical] = count\n",
    "        votes: Dict[str, Dict[str, int]] = {}\n",
    "        for cl in clusters:\n",
    "            cl_names = [mentions[idx][\"name\"] for idx in cl]\n",
    "            canonical = max(cl_names, key=len)\n",
    "            for n in cl_names:\n",
    "                votes.setdefault(n, {})\n",
    "                votes[n][canonical] = votes[n].get(canonical, 0) + 1\n",
    "                if n != canonical:\n",
    "                    logger.debug(\"Cluster [%s]: '%s' -> '%s'\", label, n, canonical)\n",
    "\n",
    "        for n, cand_counts in votes.items():\n",
    "            # choose the canonical with max votes; tie-break by longest candidate\n",
    "            best_canonical = sorted(cand_counts.items(), key=lambda kv: (kv[1], len(kv[0])), reverse=True)[0][0]\n",
    "            canonical_map[label][n] = best_canonical\n",
    "\n",
    "    # Write the canonical_map to 'canonical_map.yaml' for inspection/debugging.\n",
    "    with open('canonical_map.yaml', 'w') as f:\n",
    "        yaml.dump(canonical_map, f)\n",
    "\n",
    "    # Apply canonical surface forms to nodes and keep graph consistency.\n",
    "    # - Iterate each graph document and build a per-document id_map (old_id -> new_id).\n",
    "    # - For nodes whose label is in the configured disambiguation set:\n",
    "    #   * Replace the 'name' with its canonical form from canonical_map when different.\n",
    "    #   * Recompute the node 'id' as \"<slug(name)>::<Label>\" and update if it changes.\n",
    "    #   * Record any id change in id_map to later rewrite relationship endpoints.\n",
    "    # - After all nodes in a document are updated, update relationships with _remap_relationship_ids\n",
    "    #   using the accumulated id_map so edges still point to the correct nodes.\n",
    "    # - Track counts of name/id updates and relationship endpoint rewrites; accumulate updated docs.\n",
    "    updated_docs = []\n",
    "    total_id_changes = total_name_changes = total_rel_src_changes = total_rel_tgt_changes = 0\n",
    "    for doc in state[\"graph_docs\"]:\n",
    "        id_map: Dict[str, str] = {}\n",
    "        for node in getattr(doc, \"nodes\", []):\n",
    "            label = _get_node_label(node)\n",
    "            if label in labels:\n",
    "                old_name = _get_node_name(node)\n",
    "                new_name = canonical_map.get(label, {}).get(old_name, old_name)\n",
    "                if new_name and new_name != old_name:\n",
    "                    logger.info(\"Name remap [%s]: '%s' -> '%s'\", label, old_name, new_name)\n",
    "                    _set_node_name(node, new_name)\n",
    "                    total_name_changes += 1\n",
    "                old_id = _get_node_id(node) or _slugify(str(old_name))\n",
    "                new_id = _normalized_id(new_name or old_name, label)\n",
    "                if new_id != old_id:\n",
    "                    logger.info(\"ID remap   [%s]: '%s' -> '%s'\", label, old_id, new_id)\n",
    "                    _set(node, \"id\", new_id)\n",
    "                    id_map[old_id] = new_id\n",
    "                    total_id_changes += 1\n",
    "        rel_stats = _remap_relationship_ids(doc, id_map)\n",
    "        total_rel_src_changes += rel_stats[\"rel_src\"]\n",
    "        total_rel_tgt_changes += rel_stats[\"rel_tgt\"]\n",
    "        updated_docs.append(doc)\n",
    "\n",
    "    logger.info(\n",
    "        \"Disambiguation done. name_changes=%d id_changes=%d rel_src_updates=%d rel_tgt_updates=%d\",\n",
    "        total_name_changes, total_id_changes, total_rel_src_changes, total_rel_tgt_changes,\n",
    "    )\n",
    "    return {\"graph_docs\": updated_docs, \"canonical_map\": canonical_map}\n",
    "\n",
    "\n",
    "# Wikipedia grounding\n",
    "\n",
    "def ground_with_wikipedia(state: KGState) -> KGState:\n",
    "    labels = state.get(\"ground_labels\") or [\"Organization\", \"Product\"]\n",
    "    lang = state.get(\"wiki_language\") or CFG.wiki_lang_default\n",
    "    k = int(state.get(\"wiki_search_k\") or 5)\n",
    "    use_deepcat = bool(state.get(\"wiki_use_deepcat\") or False)\n",
    "    threshold = float(state.get(\"wiki_match_threshold\") or 0.9)\n",
    "    hints_map: Dict[str, str] = state.get(\"wiki_query_hints\") or {}\n",
    "\n",
    "    logger.info(\n",
    "        \"Wikipedia grounding start: labels=%s lang=%s k=%d threshold=%.2f\",\n",
    "        labels, lang, k, threshold,\n",
    "    )\n",
    "\n",
    "    updated_docs = []\n",
    "    total_name_changes = total_id_changes = total_rel_src_changes = total_rel_tgt_changes = grounded_nodes_count = 0\n",
    "    docs = state.get(\"docs\") or []\n",
    "\n",
    "    for idx, gd in enumerate(tqdm(state[\"graph_docs\"], desc=\"Wikipedia grounding\", unit=\"doc\")):\n",
    "        id_map: Dict[str, str] = {}\n",
    "        evidence = getattr(docs[idx], \"page_content\", \"\") if idx < len(docs) else \"\"\n",
    "        _ = evidence  # currently unused, kept for future heuristics\n",
    "        for node in getattr(gd, \"nodes\", []):\n",
    "            lbl = _get_node_label(node)\n",
    "            if lbl not in labels:\n",
    "                continue\n",
    "            query = _get_node_name(node)\n",
    "            if not query:\n",
    "                continue\n",
    "            hint = hints_map.get(lbl) or DEFAULT_WIKI_CATEGORY.get(lbl)\n",
    "            results = wiki_search_titles(query, lang=lang, limit=k, hint=hint, use_deepcat=use_deepcat)\n",
    "            best = _best_wiki_match(query, results, threshold)\n",
    "            if not best:\n",
    "                continue\n",
    "            title = best.get(\"title\") or query\n",
    "            info = wiki_page_info(title, lang=lang)\n",
    "            fullurl = info.get(\"fullurl\") or f\"https://{lang}.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "            pageid = info.get(\"pageid\") or best.get(\"pageid\")\n",
    "\n",
    "            old_name = _get_node_name(node)\n",
    "            props = _ensure_node_props(node)\n",
    "            props.update({\n",
    "                \"wiki_title\": title,\n",
    "                \"wiki_pageid\": pageid,\n",
    "                \"wiki_url\": fullurl,\n",
    "                \"wiki_lang\": lang,\n",
    "            })\n",
    "            if info.get(\"description\") is not None:\n",
    "                props[\"wiki_description\"] = info.get(\"description\")\n",
    "            if info.get(\"wikidata_id\") is not None:\n",
    "                props[\"wikidata_id\"] = info.get(\"wikidata_id\")\n",
    "\n",
    "            logger.info(\n",
    "                \"Enriched wiki [%s]: name='%s', title='%s', pageid=%s, qid=%s, url=%s (hint=%s)\",\n",
    "                lbl, old_name, title, str(pageid), str(info.get(\"wikidata_id\")), fullurl, hint,\n",
    "            )\n",
    "            grounded_nodes_count += 1\n",
    "\n",
    "            if title and title != old_name:\n",
    "                logger.info(\"Ground name [%s]: '%s' -> '%s'\", lbl, old_name, title)\n",
    "                _set_node_name(node, title)\n",
    "                total_name_changes += 1\n",
    "            old_id = _get_node_id(node) or _slugify(str(old_name))\n",
    "            new_id = _normalized_id(title or old_name, lbl)\n",
    "            if new_id != old_id:\n",
    "                logger.info(\"Ground id   [%s]: '%s' -> '%s'\", lbl, old_id, new_id)\n",
    "                _set(node, \"id\", new_id)\n",
    "                id_map[old_id] = new_id\n",
    "                total_id_changes += 1\n",
    "\n",
    "        rel_stats = _remap_relationship_ids(gd, id_map)\n",
    "        total_rel_src_changes += rel_stats[\"rel_src\"]\n",
    "        total_rel_tgt_changes += rel_stats[\"rel_tgt\"]\n",
    "        updated_docs.append(gd)\n",
    "\n",
    "    logger.info(\n",
    "        \"Wikipedia grounding done. grounded_nodes=%d name_changes=%d id_changes=%d rel_src_updates=%d rel_tgt_updates=%d\",\n",
    "        grounded_nodes_count, total_name_changes, total_id_changes, total_rel_src_changes, total_rel_tgt_changes,\n",
    "    )\n",
    "    return {\"graph_docs\": updated_docs}\n",
    "\n",
    "\n",
    "# Neo4j ingestions\n",
    "\n",
    "_NEO4J_GRAPH_SINGLETON: Optional[Neo4jGraph] = None\n",
    "\n",
    "def _get_neo4j_graph() -> Neo4jGraph:\n",
    "    global _NEO4J_GRAPH_SINGLETON\n",
    "    if _NEO4J_GRAPH_SINGLETON is None:\n",
    "        for k in (\"NEO4J_URI\", \"NEO4J_USER\", \"NEO4J_PASSWORD\"):\n",
    "            if not os.environ.get(k):\n",
    "                raise EnvironmentError(f\"Missing environment variable: {k}\")\n",
    "        _NEO4J_GRAPH_SINGLETON = Neo4jGraph(\n",
    "            url=os.environ[\"NEO4J_URI\"],\n",
    "            username=os.environ[\"NEO4J_USER\"],\n",
    "            password=os.environ[\"NEO4J_PASSWORD\"],\n",
    "        )\n",
    "        _NEO4J_GRAPH_SINGLETON.query(\n",
    "            \"\"\"\n",
    "            CREATE CONSTRAINT entity_id_unique IF NOT EXISTS\n",
    "            FOR (n:Entity) REQUIRE n.id IS UNIQUE\n",
    "            \"\"\"\n",
    "        )\n",
    "    return _NEO4J_GRAPH_SINGLETON\n",
    "\n",
    "\n",
    "def write_to_neo4j(state: KGState) -> KGState:\n",
    "    graph = _get_neo4j_graph()\n",
    "    graph.add_graph_documents(state[\"graph_docs\"], include_source=True)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def sanity_checks(state: KGState) -> KGState:\n",
    "    graph = _get_neo4j_graph()\n",
    "    node_rows = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (n:Entity)\n",
    "        RETURN\n",
    "          n.name AS name,\n",
    "          n.wiki_title AS wiki_title,\n",
    "          n.wikidata_id AS wikidata_id,\n",
    "          n.wiki_url AS wiki_url,\n",
    "          n.wiki_pageid AS wiki_pageid,\n",
    "          n.wiki_lang AS wiki_lang,\n",
    "          n.wiki_description AS wiki_description,\n",
    "          labels(n) AS labels\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    rel_rows = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (a)-[r]->(b)\n",
    "        RETURN a.name AS from, TYPE(r) AS rel, b.name AS to, r.source AS source\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "    )\n",
    "    wiki_counts = graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (n:Entity)\n",
    "        RETURN\n",
    "          count(n) AS total_nodes,\n",
    "          sum(CASE WHEN n.wiki_title IS NOT NULL THEN 1 ELSE 0 END) AS grounded_nodes,\n",
    "          sum(CASE WHEN n.wikidata_id IS NOT NULL THEN 1 ELSE 0 END) AS with_wikidata\n",
    "        \"\"\"\n",
    "    )\n",
    "    return {\"node_samples\": node_rows, \"rel_samples\": rel_rows, \"wiki_counts\": wiki_counts}\n",
    "\n",
    "\n",
    "# Graph wiring\n",
    "\n",
    "builder = StateGraph(KGState)\n",
    "builder.add_node(\"build_docs\", build_docs)\n",
    "builder.add_node(\"chunk_docs\", chunk_docs)\n",
    "builder.add_node(\"extract_graph\", extract_graph)\n",
    "# validation\n",
    "builder.add_node(\"validate_with_judge\", validate_with_judge)\n",
    "builder.add_node(\"disambiguate_entities\", disambiguate_entities)\n",
    "builder.add_node(\"ground_with_wikipedia\", ground_with_wikipedia)\n",
    "# io\n",
    "builder.add_node(\"write_to_neo4j\", write_to_neo4j)\n",
    "builder.add_node(\"sanity_checks\", sanity_checks)\n",
    "\n",
    "builder.add_edge(START, \"build_docs\")\n",
    "builder.add_edge(\"build_docs\", \"chunk_docs\")\n",
    "builder.add_edge(\"chunk_docs\", \"extract_graph\")\n",
    "# judge first, then disambiguation, then grounding\n",
    "builder.add_edge(\"extract_graph\", \"validate_with_judge\")\n",
    "builder.add_edge(\"validate_with_judge\", \"disambiguate_entities\")\n",
    "# builder.add_edge(\"disambiguate_entities\", \"ground_with_wikipedia\")\n",
    "# builder.add_edge(\"ground_with_wikipedia\", \"write_to_neo4j\")\n",
    "builder.add_edge(\"disambiguate_entities\", \"write_to_neo4j\")\n",
    "builder.add_edge(\"write_to_neo4j\", \"sanity_checks\")\n",
    "builder.add_edge(\"sanity_checks\", END)\n",
    "\n",
    "workflow = builder.compile()\n",
    "\n",
    "\n",
    "# Demo: load HF dataset and run\n",
    "\n",
    "def load_bloomberg_texts(max_items: int = 200, seed: int = 42, keywords: Optional[List[str]] = None) -> List[str]:\n",
    "    \"\"\"Load Bloomberg pretraining dataset, clean, keyword-filter, and sample texts.\"\"\"\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"HuggingFace 'datasets' is required. Install with: pip install datasets\"\n",
    "        ) from e\n",
    "\n",
    "    ds = load_dataset(\"Aletheia-ng/bloomberg-news-articles-pretraining-dataset\")\n",
    "    split_name = \"train\" if \"train\" in ds else next(iter(ds.keys()))\n",
    "    split = ds[split_name]\n",
    "\n",
    "    raw_texts = [t for t in split[\"text\"] if isinstance(t, str) and t.strip()]\n",
    "\n",
    "    # Truncate boilerplate like: \"To contact the reporters on this story ...\"\n",
    "    cleaned = [re.split(r\"\bTo contact the reporters? on this story\b\", t)[0].strip() for t in raw_texts]\n",
    "\n",
    "    kws = keywords or [\"Apple\", \"Google\", \"Microsoft\", \"Amazon\"]\n",
    "    pat = re.compile(\"|\".join(map(re.escape, kws)), re.IGNORECASE)\n",
    "    filtered = [t for t in cleaned if pat.search(t)]\n",
    "\n",
    "    if not filtered:\n",
    "        raise ValueError(\"No texts matched the keyword filter; adjust 'keywords'.\")\n",
    "\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    sampled = random.sample(filtered, min(max_items, len(filtered)))\n",
    "    logger.info(\"Selected %d texts for processing out of %d after keyword filter (%s)\", len(sampled), len(filtered), \", \".join(kws))\n",
    "    return sampled\n",
    "\n",
    "\n",
    "# Entry point\n",
    "\n",
    "def run_workflow(\n",
    "    sentences: List[str],\n",
    "    *,\n",
    "    chunk_target_bytes: Optional[int] = None,\n",
    "    disambiguate_labels: Optional[List[str]] = None,\n",
    "    disambiguation_threshold: Optional[float] = None,\n",
    "    ground_labels: Optional[List[str]] = None,\n",
    "    wiki_language: Optional[str] = None,\n",
    "    wiki_search_k: Optional[int] = None,\n",
    "    wiki_match_threshold: Optional[float] = None,\n",
    "    wiki_use_deepcat: Optional[bool] = None,\n",
    "    wiki_query_hints: Optional[Dict[str, str]] = None,\n",
    "    judge_batch_size: Optional[int] = None,\n",
    "    judge_node_threshold: Optional[float] = None,\n",
    "    judge_rel_threshold: Optional[float] = None,\n",
    "    judge_model_name: Optional[str] = None,\n",
    "    judge_action: Optional[str] = None,\n",
    ") -> KGState:\n",
    "    pipeline_setup: KGState = {\"sentences\": sentences}\n",
    "\n",
    "    if chunk_target_bytes is not None: pipeline_setup[\"chunk_target_bytes\"] = chunk_target_bytes\n",
    "    if disambiguate_labels is not None: pipeline_setup[\"disambiguate_labels\"] = disambiguate_labels\n",
    "    if disambiguation_threshold is not None: pipeline_setup[\"disambiguation_threshold\"] = disambiguation_threshold\n",
    "    if ground_labels is not None: pipeline_setup[\"ground_labels\"] = ground_labels\n",
    "    if wiki_language is not None: pipeline_setup[\"wiki_language\"] = wiki_language\n",
    "    if wiki_search_k is not None: pipeline_setup[\"wiki_search_k\"] = wiki_search_k\n",
    "    if wiki_use_deepcat is not None: pipeline_setup[\"wiki_use_deepcat\"] = wiki_use_deepcat\n",
    "    if wiki_match_threshold is not None: pipeline_setup[\"wiki_match_threshold\"] = wiki_match_threshold\n",
    "    if wiki_query_hints is not None: pipeline_setup[\"wiki_query_hints\"] = wiki_query_hints\n",
    "    if judge_batch_size is not None: pipeline_setup[\"judge_batch_size\"] = judge_batch_size\n",
    "    if judge_node_threshold is not None: pipeline_setup[\"judge_node_threshold\"] = judge_node_threshold\n",
    "    if judge_rel_threshold is not None: pipeline_setup[\"judge_rel_threshold\"] = judge_rel_threshold\n",
    "    if judge_model_name is not None: pipeline_setup[\"judge_model_name\"] = judge_model_name\n",
    "    if judge_action is not None: pipeline_setup[\"judge_action\"] = judge_action\n",
    "\n",
    "    result_state = workflow.invoke(pipeline_setup)\n",
    "    summary = result_state.get(\"summary\", {})\n",
    "    print(f\"Extracted {summary.get('node_count', 0)} nodes and {summary.get('relationship_count', 0)} relationships.\")\n",
    "    if result_state.get(\"canonical_map\"):\n",
    "        print(\"\\nDisambiguation applied for labels:\")\n",
    "        for lbl, mp in result_state[\"canonical_map\"].items():\n",
    "            if mp:\n",
    "                print(f\"- {lbl}: {len(mp)} names mapped\")\n",
    "\n",
    "    return result_state\n",
    "\n",
    "\n",
    "def run_demo_from_hf_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Convenience runner that pulls a sample from HF and executes the workflow.\"\"\"\n",
    "    sampled_texts = load_bloomberg_texts(max_items=500, seed=42,\n",
    "                                         keywords=[\"Apple\", \"Google\", \"Microsoft\", \"Amazon\"]) \n",
    "    print(f\"Selected {len(sampled_texts)} texts for processing.\")\n",
    "\n",
    "    state = run_workflow(\n",
    "        sampled_texts,\n",
    "        chunk_target_bytes=2048,\n",
    "        disambiguate_labels=[\"Product\", \"Organization\", \"Country\", \"Government Body\", \"Person\", \"Location\", \"City\"],\n",
    "        disambiguation_threshold=1.0,\n",
    "        ground_labels=[\"Product\", \"Organization\", \"Country\", \"Government Body\", \"Person\", \"Location\", \"City\"],\n",
    "        wiki_language=\"en\",\n",
    "        wiki_search_k=50,\n",
    "        wiki_use_deepcat=False,\n",
    "        wiki_match_threshold=0.9,  # works well for title similarity; reranker path also uses this threshold\n",
    "        judge_batch_size=8,\n",
    "        judge_node_threshold=0.3,\n",
    "        judge_rel_threshold=0.3,\n",
    "        judge_model_name=\"google/flan-t5-xl\",\n",
    "        judge_action=\"annotate\",  # set to 'filter' to drop low-confidence items\n",
    "    )\n",
    "    \n",
    "    return state.get('judge_results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3dca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 500 texts for processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868e477de92a4ddbb5e9ce85aa83ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk docs:   0%|          | 0/500 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2e3a432d5b4f39846b8ec8f5250746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extract graph:   0%|          | 0/1860 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884c7e577ace4a4887ebe8cf3533e7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922832677acc49e8b19d1737f479833e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judge validation:   0%|          | 0/1860 [00:00<?, ?doc/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (533 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "judge_results = run_demo_from_hf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert judge results to a DataFrame for easier inspection\n",
    "df = pd.DataFrame(judge_results)\n",
    "\n",
    "# Save to a CSV\n",
    "df.to_csv('judge_results.csv', index=False)\n",
    "\n",
    "# Show summary statistics\n",
    "total_judged = len(df)\n",
    "accepted = df['accepted'].sum()\n",
    "rejected = total_judged - accepted\n",
    "print(f\"\\nTotal judged items: {total_judged}\")\n",
    "print(f\"Accepted: {accepted}\")\n",
    "print(f\"Rejected: {rejected}\")\n",
    "\n",
    "# Show the columns of the DataFrame\n",
    "print(\"\\nColumns in the judge results DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute acceptance ratio and counts\n",
    "summary = (\n",
    "    df.groupby(\"type\")\n",
    "      .agg(acceptance_ratio=(\"accepted\", \"mean\"),\n",
    "           count=(\"accepted\", \"size\"))\n",
    "      .reset_index()\n",
    "      .sort_values(\"acceptance_ratio\", ascending=False)\n",
    ")\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(\n",
    "    data=summary,\n",
    "    x=\"type\", y=\"acceptance_ratio\",\n",
    "    palette=\"viridis\",\n",
    "    hue=\"type\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Above Judge Threshold Ratio\", fontsize=14)\n",
    "ax.set_xlabel(\"Type\")\n",
    "ax.set_ylabel(\"Above Judge Threshold Ratio\")\n",
    "ax.set_ylim(0, 1.1)  # leave room for labels\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    df,\n",
    "    col=\"type\",\n",
    "    col_wrap=3,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    height=4,      # height of each subplot in inches\n",
    "    aspect=1.2     # width = height * aspect\n",
    ")\n",
    "g.map_dataframe(sns.histplot, x=\"score\", color=\"skyblue\", bins=20)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Score\", \"Count\")\n",
    "g.fig.suptitle(\"Judge Score Distribution by Type\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"type\", y=\"score\",\n",
    "    palette=\"viridis\",\n",
    "    hue=\"type\"\n",
    ")\n",
    "plt.title(\"Judge Score Distribution by Type\", fontsize=14)\n",
    "plt.xlabel(\"Type\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa578d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_node = df[df[\"type\"] == \"node\"]\n",
    "\n",
    "# plot histograms by name\n",
    "g = sns.FacetGrid(\n",
    "    df_node,\n",
    "    col=\"name\",\n",
    "    col_wrap=3,       # adjust for how many unique names you have\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    height=3,\n",
    "    aspect=1.2\n",
    ")\n",
    "g.map_dataframe(sns.histplot, x=\"score\", color=\"steelblue\", bins=20)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Score\", \"Count\")\n",
    "g.fig.suptitle(\"Judge Score Distribution by Name (type=node)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_node = df[df[\"type\"] == \"relation\"]\n",
    "\n",
    "# plot histograms by name\n",
    "g = sns.FacetGrid(\n",
    "    df_node,\n",
    "    col=\"name\",\n",
    "    col_wrap=3,       # adjust for how many unique names you have\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    height=3,\n",
    "    aspect=1.2\n",
    ")\n",
    "g.map_dataframe(sns.histplot, x=\"score\", color=\"steelblue\", bins=20)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_axis_labels(\"Score\", \"Count\")\n",
    "g.fig.suptitle(\"Judge Score Distribution by Name (type=relation)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canonicalizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
